<!doctype html>
<html lang="zh-CN">
<head>

    <meta charset="utf-8">
    <meta name="generator" content="Hugo 0.57.0" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>ai | Learning Hardy</title>
    <meta property="og:title" content="ai - Learning Hardy">
    <meta property="og:type" content="article">
        
        
    <meta name="Keywords" content="">
    <meta name="description" content="ai">
        
    <meta name="author" content="">
    <meta property="og:url" content="https://hardy5012.github.io/categories/ai/">
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">

    <link rel="stylesheet" href="/css/normalize.css">
    
    <link rel="stylesheet" href="/css/style.css">
    <link rel="alternate" type="application/rss+xml+xml" href="https://hardy5012.github.io/categories/ai/index.xml" title="Learning Hardy" />
    <script type="text/javascript" src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script>

    


    
    
</head>


<body>
<header id="header" class="clearfix">
    <div class="container">
        <div class="col-group">
            <div class="site-name ">
                
                    <a id="logo" href="https://hardy5012.github.io/">
                        Learning Hardy
                    </a>
                
                
            </div>
            <div>
                <nav id="nav-menu" class="clearfix">
                    <a class="" href="https://hardy5012.github.io/">首页</a>
                    
                    <a  href="https://hardy5012.github.io/archives/" title="归档">归档</a>
                    
                    <a  href="https://hardy5012.github.io/about/" title="关于">关于</a>
                    
                </nav>
            </div>
        </div>
    </div>
</header>


<div id="body">
    <div class="container">
        <div class="col-group">

            <div class="col-8" id="main">
                <div class="res-cons">
                    
                    <h3 class="archive-title">
                        分类
                        <span class="keyword">ai</span>
                        中的文章
                    </h3>
                    

                    
                        <article class="post">
                            <header>
                                <h1 class="post-title">
                                    <a href="https://hardy5012.github.io/post/machine-learning4-regularization/">Machine Learning(4) Regularization</a>
                                </h1>
                            </header>
                            <date class="post-meta meta-date">
                                2017年5月17日
                            </date>
                            
                            <div class="post-meta meta-category">
                                |
                                
                                    <a href="https://hardy5012.github.io/categories/ai">ai</a>
                                
                            </div>
                            
                            <div class="post-content">
                                正规化(Regularization)弱化的高阶项的系数，这弱化也称为对参数θ的惩罚（penalize）。 线性回归中的正规化   公式变为： \begin{gather*} J(θ) &amp;= \frac{1}{2m}∑limitsi=1^m(h_θ(x(i))-y(i))^2+λ∑limitsi=1nθ_j^2 &amp;= \frac{1}{2m}(Xθ-y)^T(Xθ-y)+λ∑limitsi=1nθ_j^2 \end{gather*} 其中，参数λ主要是完成以下两个任务:    证对数据的拟合良好    保证θ足够小，避免过拟合问题    λ越大，要使 J(θ)变小，惩罚力度就要变大，这样θ会被惩罚得越惨（越小），即要避免过拟合，我们显然应当增大λ的值。  梯度下降公式变为： Repeat{ \begin{gather*} θ_0 &amp;=θ_0-α\frac{1}{m}∑limitsi=1m(h_θ(x(i))-y(i))x_0(i) \\  θ_j &amp;=θ_j-α\big(\frac{1}{m}∑limitsi=1m(h_θ(x(i))-y(i))x_j(i)+\frac{λ}{m}θ_j\big) \quad (1) \\  \mbox {即：}&amp; θ &amp;= θ-α*(\frac{1}{m} X^T(y-Xθ) + \frac{λ}{m}θj) \quad j ≠q 0  \end{gather*}  } 其中，（1）式等价于： $$ θ_j=θ_j(1-α\frac{λ}{m})-α\frac{1}{m}∑limitsi=1m[h_θ(x(i))-y(i)]x_j(i) $$ 由于 $ 1-α\frac{λ}{m}&lt;1 $，故而梯度下降中每次更新θ ，同时也会去减小θ值，达到了 Regularization 的目的。  正规方程： $$ θ=(X^TX+λ≤ft[\begin{array}{ccccc}0 &amp;⋯ &amp;⋯ &amp;⋯ &amp;0 \\ 0 &amp;1 &amp;⋯ &amp;⋯ &amp;0\\ \vdots &amp; \vdots &amp; 1 &amp;⋯ &amp; 0\\ \vdots &amp;\vdots &amp;⋯ &amp;\ddots &amp; \vdots \\ 0 &amp; 0 &amp;⋯ &amp;⋯ &amp;1 \end{array}\right])-1X^Ty $$ 正规化同时可以解决矩阵不可逆的问题（ $X^TX$ 不可逆, $X^TX + λ.……
                                <p class="readmore"><a href="https://hardy5012.github.io/post/machine-learning4-regularization/">阅读全文</a></p>
                            </div>
                        </article>
                    
                        <article class="post">
                            <header>
                                <h1 class="post-title">
                                    <a href="https://hardy5012.github.io/post/2017-05-07-machine-learning%E4%B8%89-neural-networks/">Machine Learning(3) Neural Networks</a>
                                </h1>
                            </header>
                            <date class="post-meta meta-date">
                                2017年5月7日
                            </date>
                            
                            <div class="post-meta meta-category">
                                |
                                
                                    <a href="https://hardy5012.github.io/categories/ai">ai</a>
                                
                            </div>
                            
                            <div class="post-content">
                                Model Representation  \text {If network has $s_j$ units in layer $j$ and $sj+1$ units in layer $j+1$, then $Θ(j)$ will be of dimension $sj+1 × (s_j + 1)$.}  Cost Function   Let&#39;s first define a few variables that we will need to use: L = total number of layers in the network $s_l$ = number of units (not counting bias unit) in layer l K = number of output units/classes   \begin{gather*} J(Θ) = - \frac{1}{m} ∑i=1^m ∑k=1^K ≤ft[y(i)_k log ((h_Θ (x(i)))_k) + (1 - y(i)_k)log (1 - (h_Θ(x(i)))_k)\right] + \frac{λ}{2m}∑l=1L-1 ∑i=1s_l ∑j=1^{sl+1} ( Θj,i(l))^2\end{gather*} Gradient Checking   Gradient checking will assure that our backpropagation works as intended.……
                                <p class="readmore"><a href="https://hardy5012.github.io/post/2017-05-07-machine-learning%E4%B8%89-neural-networks/">阅读全文</a></p>
                            </div>
                        </article>
                    
                        <article class="post">
                            <header>
                                <h1 class="post-title">
                                    <a href="https://hardy5012.github.io/post/2017-04-27-machine-learing%E4%BA%8C-logistic-regression/">Machine Learning(2）Logistic Regression</a>
                                </h1>
                            </header>
                            <date class="post-meta meta-date">
                                2017年4月27日
                            </date>
                            
                            <div class="post-meta meta-category">
                                |
                                
                                    <a href="https://hardy5012.github.io/categories/ai">ai</a>
                                
                            </div>
                            
                            <div class="post-content">
                                 Decision Boundary   决策边界(Decison Boundary)就是用来划清界限的边界，即属于哪一类。 “决策边界是预测函数$h_θ(x)$ 的属性，而不是训练集属性”，这是因为能作出“划清”类间界限的只有$h_θ(x)$ ，而训练集只是用来训练和调节参数的。 Hypothesis   在逻辑回归中，定义预测函数为: $$h_θ (x) = g(z)$$ 其中，$z=θ^Tx$ 是分类边界，且 $g(z)=\frac{1}{1+e-z}$ Cost Function   \begin{gather*} &amp; J(θ) = \dfrac{1}{m} ∑i=1^m \mathrm{Cost}(h_θ(x(i)),y(i)) ≠wline &amp; \mathrm{Cost}(h_θ(x),y) = -log(h_θ(x)) \; &amp; \text{if y = 1} ≠wline &amp; \mathrm{Cost}(h_θ(x),y) = -log(1-h_θ(x)) \; &amp; \text{if y = 0} \end{gather*}  \begin{gather*} &amp; \mathrm{Cost}(h_θ(x),y) = 0 \text{ if } h_θ(x) = y ≠wline &amp; \mathrm{Cost}(h_θ(x),y) → ∞ \text{ if } y = 0 \; \mathrm{and} \; h_θ(x) → 1 ≠wline &amp; \mathrm{Cost}(h_θ(x),y) → ∞ \text{ if } y = 1 \; \mathrm{and} \; h_θ(x) → 0 ≠wline \end{gather*} Simplified Cost Function   $$\mathrm{Cost}(h_θ(x),y) = - y \; log(h_θ(x)) - (1 - y) log(1 - h_θ(x))$$ $$J(θ) = - \frac{1}{m} \displaystyle ∑i=1^m [y(i)log (h_θ (x(i))) + (1 - y(i))log (1 - h_θ(x(i)))]$$ \begin{gather*} &amp; h = g(Xθ)≠wline &amp; J(θ) = \frac{1}{m} ⋅ ≤ft(-yTlog(h)-(1-y)Tlog(1-h)\right) \end{gather*} Gradient Descent   \begin{gather*}&amp; Repeat \; \lbrace ≠wline &amp; \; θ_j := θ_j - α \dfrac{∂}{∂ θ_j}J(θ) ≠wline &amp; \rbrace\end{gather*} \begin{gather*} &amp; Repeat \; \lbrace ≠wline &amp; \; θ_j := θ_j - \frac{α}{m} ∑i=1^m (h_θ(x(i)) - y(i)) x_j(i) ≠wline &amp; \rbrace \end{gather*} A vectorized implementation is: $$θ := θ - \frac{α}{m} XT (g(X θ ) - \vec{y})$$ Multi-class classification   多分类问题通常采用 One-vs-All ，亦称 One-vs-the Rest 方法来实现多分类，其将多分类问题转化为了多次二分类问题 overfitting   过拟合：如果太多的特征时，预测函数能拟合训练集，即代价函数的值非常小，但无法正确预测新的新的样本。 解决方案：    减少特征    正规化   ……
                                <p class="readmore"><a href="https://hardy5012.github.io/post/2017-04-27-machine-learing%E4%BA%8C-logistic-regression/">阅读全文</a></p>
                            </div>
                        </article>
                    
                        <article class="post">
                            <header>
                                <h1 class="post-title">
                                    <a href="https://hardy5012.github.io/post/2017-04-21-maching-learing-normal-equation/">Machine Learning(1)Linear Regression</a>
                                </h1>
                            </header>
                            <date class="post-meta meta-date">
                                2017年4月21日
                            </date>
                            
                            <div class="post-meta meta-category">
                                |
                                
                                    <a href="https://hardy5012.github.io/categories/ai">ai</a>
                                
                            </div>
                            
                            <div class="post-content">
                                 Hypothesis   $$ h_θ(x)=θ_0+θ_1x_1+θ_2x_2+…+θ_nx_n=θ^Tx $$ Cost Function   $$ J(θ)=\frac{1}{2m}∑limitsi=1m(h_θ(x(i))-y(i))^2,\quad \mbox{$m$ 为样本数} $$ Normal Equation   $$ θ = (X^TX)-1X^Ty $$ 在特征值少于 1w 时则可以使用标准方程一次得解，但如果特征值过多时，需要使用梯度下降方程进行求解. 标准方程的复杂度为$\mathrm{O}(n^3)$, 梯度下降为$\mathrm{O}(n^2)$  $(X^TX)-1$ 不可逆的情况：    冗余的特征，如线性依赖    太多特征，样本小于特征   ……
                                <p class="readmore"><a href="https://hardy5012.github.io/post/2017-04-21-maching-learing-normal-equation/">阅读全文</a></p>
                            </div>
                        </article>
                    
                        <article class="post">
                            <header>
                                <h1 class="post-title">
                                    <a href="https://hardy5012.github.io/post/machine-learning0-introduction/">Machine Learning(0) introduction</a>
                                </h1>
                            </header>
                            <date class="post-meta meta-date">
                                2017年4月13日
                            </date>
                            
                            <div class="post-meta meta-category">
                                |
                                
                                    <a href="https://hardy5012.github.io/categories/ai">ai</a>
                                
                            </div>
                            
                            <div class="post-content">
                                 Supervised Learning and Unsupevised Learning     监督学习（Supervised Learning）所给的样本已知道正确的输出是什么样子，对输入与输出有一定的了解。    无监督学习（Unsupervised Learning）能够在很少或根本不知道结果应该是什么样的情况下解决问题。通过数据变量之间的关系数据聚类推导出这种结构。    在无监督学习下，没有基于预测结果的反馈。 Regression and Classification     归回问题（Regression）预测一个连续的输出，也就说把输入映射到连续函数中。    分类问题(Classification)以离散输出预测结果。换句话说，将变映射到离散类别。   Cost Function   代价函数 (cost function)用来衡量预测函数的准确程度。 Gradient descent   梯度下降 (Gradient descent)使用代价函数最小化。 $$ θ_j = θ_j-α\frac{∂}{∂θ_j}J(θ) \quad \mbox{$α$ 为学习率} $$ $$ \begin{align*} \text{repeat until convergence: } \lbrace &amp; ≠wline θ_0 := &amp; θ_0 - α \frac{1}{m} ∑limitsi=1m(h_θ(xi) - yi) ≠wline θ_1 := &amp; θ_1 - α \frac{1}{m} ∑limitsi=1m≤ft((h_θ(xi) - yi) xi\right) ≠wline \rbrace&amp; \end{align*} $$    注意在更新$θ$时需要同步更新。    训练中不用初时高速学习速率，因为在接近局部最小值时。导数的绝对值会变小，这样步幅自然小了。    训练就是不断的梯度下降直到收敛，导数为 0, 梯度下降中$θ$不在变化。    对特征缩放和均一化可以提高训练。   ……
                                <p class="readmore"><a href="https://hardy5012.github.io/post/machine-learning0-introduction/">阅读全文</a></p>
                            </div>
                        </article>
                    

                    





                </div>
            </div>

            <div id="secondary">
    <section class="widget">
        <form id="search" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" _lpchecked="1">
      
      <input type="text" name="q" maxlength="20" placeholder="Search">
      <input type="hidden" name="sitesearch" value="https://hardy5012.github.io/">
      <button type="submit" class="submit icon-search"></button>
</form>
    </section>
    
    <section class="widget">
        <h3 class="widget-title">最近文章</h3>
<ul class="widget-list">
    
    <li>
        <a href="https://hardy5012.github.io/post/%E4%B8%BA%E8%BF%90%E8%A1%8C%E7%9A%84%E5%AE%B9%E5%99%A8%E6%8C%82%E8%BD%BD%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95/" title="为运行的容器挂载文件目录">为运行的容器挂载文件目录</a>
    </li>
    
    <li>
        <a href="https://hardy5012.github.io/post/super%E6%B2%A1%E9%82%A3%E4%B9%88%E7%AE%80%E5%8D%95/" title="super 没那么简单">super 没那么简单</a>
    </li>
    
    <li>
        <a href="https://hardy5012.github.io/post/postgres%E4%B8%AD%E7%9A%84%E4%BA%94%E7%A7%8D%E5%88%86%E9%A1%B5%E6%96%B9%E5%BC%8F/" title="Postgres 中的五种分页方式">Postgres 中的五种分页方式</a>
    </li>
    
    <li>
        <a href="https://hardy5012.github.io/post/c&#43;&#43;-ep23-if-constexpt/" title="C&#43;&#43; Ep23: constexpr if">C&#43;&#43; Ep23: constexpr if</a>
    </li>
    
    <li>
        <a href="https://hardy5012.github.io/post/c&#43;&#43;-ep22-fibonacci/" title="C&#43;&#43; Ep22: Fibonacci">C&#43;&#43; Ep22: Fibonacci</a>
    </li>
    
    <li>
        <a href="https://hardy5012.github.io/post/c&#43;&#43;-ep21-variadic-templates/" title="C&#43;&#43; Ep21: variadic Templates">C&#43;&#43; Ep21: variadic Templates</a>
    </li>
    
    <li>
        <a href="https://hardy5012.github.io/post/c&#43;&#43;-ep20-static-variable/" title="C&#43;&#43; Ep20: Static Variable">C&#43;&#43; Ep20: Static Variable</a>
    </li>
    
    <li>
        <a href="https://hardy5012.github.io/post/tweaks2/" title="Tweaks(2)">Tweaks(2)</a>
    </li>
    
    <li>
        <a href="https://hardy5012.github.io/post/tweaks1/" title="Tweaks(1)">Tweaks(1)</a>
    </li>
    
    <li>
        <a href="https://hardy5012.github.io/post/c&#43;&#43;-ep19the-concurrency-api4/" title="C&#43;&#43; Ep19:The Concurrency API(4)">C&#43;&#43; Ep19:The Concurrency API(4)</a>
    </li>
    
</ul>
    </section>

    

    <section class="widget">
        <h3 class="widget-title">分类</h3>
<ul class="widget-list">
    
    <li>
        <a href="https://hardy5012.github.io/categories/ai/">ai(5)</a>
    </li>
    
    <li>
        <a href="https://hardy5012.github.io/categories/c&#43;&#43;/">c&#43;&#43;(20)</a>
    </li>
    
    <li>
        <a href="https://hardy5012.github.io/categories/date/">date(1)</a>
    </li>
    
    <li>
        <a href="https://hardy5012.github.io/categories/modern/">modern(16)</a>
    </li>
    
    <li>
        <a href="https://hardy5012.github.io/categories/notes/">notes(10)</a>
    </li>
    
    <li>
        <a href="https://hardy5012.github.io/categories/summary/">summary(5)</a>
    </li>
    
    <li>
        <a href="https://hardy5012.github.io/categories/weekly/">weekly(4)</a>
    </li>
    
</ul>
    </section>

    <section class="widget">
        <h3 class="widget-title">标签</h3>
<div class="tagcloud">
    
    <a href="https://hardy5012.github.io/tags/boost/">boost</a>
    
    <a href="https://hardy5012.github.io/tags/c&#43;&#43;/">c&#43;&#43;</a>
    
    <a href="https://hardy5012.github.io/tags/constexpr/">constexpr</a>
    
    <a href="https://hardy5012.github.io/tags/docker/">docker</a>
    
    <a href="https://hardy5012.github.io/tags/etf/">etf</a>
    
    <a href="https://hardy5012.github.io/tags/fibonacci/">fibonacci</a>
    
    <a href="https://hardy5012.github.io/tags/git/">git</a>
    
    <a href="https://hardy5012.github.io/tags/gprof/">gprof</a>
    
    <a href="https://hardy5012.github.io/tags/heroku/">heroku</a>
    
    <a href="https://hardy5012.github.io/tags/linear/">linear</a>
    
    <a href="https://hardy5012.github.io/tags/linux/">linux</a>
    
    <a href="https://hardy5012.github.io/tags/logistic/">logistic</a>
    
    <a href="https://hardy5012.github.io/tags/makefile/">makefile</a>
    
    <a href="https://hardy5012.github.io/tags/neural/">neural</a>
    
    <a href="https://hardy5012.github.io/tags/postgrespaginate/">postgrespaginate</a>
    
    <a href="https://hardy5012.github.io/tags/protobuf/">protobuf</a>
    
    <a href="https://hardy5012.github.io/tags/python/">python</a>
    
    <a href="https://hardy5012.github.io/tags/regression/">regression</a>
    
    <a href="https://hardy5012.github.io/tags/regularization/">regularization</a>
    
    <a href="https://hardy5012.github.io/tags/spacemacs/">spacemacs</a>
    
    <a href="https://hardy5012.github.io/tags/static/">static</a>
    
    <a href="https://hardy5012.github.io/tags/template/">template</a>
    
</div>
    </section>

    

    <section class="widget">
        <h3 class="widget-title">其它</h3>
        <ul class="widget-list">
            <li><a href="https://hardy5012.github.io/index.xml">文章 RSS</a></li>
        </ul>
    </section>
</div>
        </div>
    </div>
</div>
<footer id="footer">
    <div class="container">
        &copy; 2019 <a href="https://hardy5012.github.io/">Learning Hardy By </a>.
        Powered by <a rel="nofollow noreferer noopener" href="https://gohugo.io" target="_blank">Hugo</a>.
        <a href="https://www.flysnow.org/" target="_blank">Theme</a> based on <a href="https://github.com/rujews/maupassant-hugo" target="_blank">maupassant</a>.
        
    </div>
</footer>


<a id="rocket" href="#top"></a>
<script type="text/javascript" src="/js/totop.js?v=0.0.0" async=""></script>







</body>
</html>
